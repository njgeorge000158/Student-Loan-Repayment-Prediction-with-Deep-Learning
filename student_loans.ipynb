{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#*******************************************************************************************\n",
    " #\n",
    " #  File Name:  student_loans.ipynb\n",
    " #\n",
    " #  File Description:\n",
    " #      This interactive Python notebook, student_loans.ipynb, reads a csv file,\n",
    " #      student_loans.csv, and uses deep learning methods to process the features \n",
    " #      in the provided dataset and create a binary classifier that can predict \n",
    " #      whether student loan applicants will default or not.\n",
    " #\n",
    " #\n",
    " #  Date            Description                             Programmer\n",
    " #  ----------      ------------------------------------    ------------------\n",
    " #  04/15/2024      Initial Development                     Nicholas J. George\n",
    " #\n",
    " #******************************************************************************************/\n",
    "\n",
    "import logx\n",
    "import pandas_processx\n",
    "import student_loans_constants\n",
    "\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from silence_tensorflow import silence_tensorflow\n",
    "silence_tensorflow()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONSTANT_LOCAL_FILE_NAME = 'student_loans.ipynb'\n",
    "\n",
    "\n",
    "logx.set_log_mode(False)\n",
    "\n",
    "logx.set_image_mode(False)\n",
    "\n",
    "\n",
    "logx.begin_program('student_loans')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <br> **Section 1: Extraction**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1.1: Read the CSV data into a Pandas DataFrame**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "student_loan_dataframe = pd.read_csv(student_loans_constants.CONSTANT_INPUT_FILE_PATH)\n",
    "\n",
    "logx.log_write_object(student_loan_dataframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1.2: Display Student Loan DataFrame**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_3038d caption {\n",
       "  color: black;\n",
       "  font-size: 20px;\n",
       "  font-style: bold;\n",
       "  text-align: center;\n",
       "}\n",
       "#T_3038d_row0_col0, #T_3038d_row0_col1, #T_3038d_row0_col2, #T_3038d_row0_col3, #T_3038d_row0_col4, #T_3038d_row0_col5, #T_3038d_row0_col6, #T_3038d_row0_col7, #T_3038d_row0_col8, #T_3038d_row0_col9, #T_3038d_row0_col10, #T_3038d_row0_col11, #T_3038d_row1_col0, #T_3038d_row1_col1, #T_3038d_row1_col2, #T_3038d_row1_col3, #T_3038d_row1_col4, #T_3038d_row1_col5, #T_3038d_row1_col6, #T_3038d_row1_col7, #T_3038d_row1_col8, #T_3038d_row1_col9, #T_3038d_row1_col10, #T_3038d_row1_col11, #T_3038d_row2_col0, #T_3038d_row2_col1, #T_3038d_row2_col2, #T_3038d_row2_col3, #T_3038d_row2_col4, #T_3038d_row2_col5, #T_3038d_row2_col6, #T_3038d_row2_col7, #T_3038d_row2_col8, #T_3038d_row2_col9, #T_3038d_row2_col10, #T_3038d_row2_col11, #T_3038d_row3_col0, #T_3038d_row3_col1, #T_3038d_row3_col2, #T_3038d_row3_col3, #T_3038d_row3_col4, #T_3038d_row3_col5, #T_3038d_row3_col6, #T_3038d_row3_col7, #T_3038d_row3_col8, #T_3038d_row3_col9, #T_3038d_row3_col10, #T_3038d_row3_col11, #T_3038d_row4_col0, #T_3038d_row4_col1, #T_3038d_row4_col2, #T_3038d_row4_col3, #T_3038d_row4_col4, #T_3038d_row4_col5, #T_3038d_row4_col6, #T_3038d_row4_col7, #T_3038d_row4_col8, #T_3038d_row4_col9, #T_3038d_row4_col10, #T_3038d_row4_col11, #T_3038d_row5_col0, #T_3038d_row5_col1, #T_3038d_row5_col2, #T_3038d_row5_col3, #T_3038d_row5_col4, #T_3038d_row5_col5, #T_3038d_row5_col6, #T_3038d_row5_col7, #T_3038d_row5_col8, #T_3038d_row5_col9, #T_3038d_row5_col10, #T_3038d_row5_col11, #T_3038d_row6_col0, #T_3038d_row6_col1, #T_3038d_row6_col2, #T_3038d_row6_col3, #T_3038d_row6_col4, #T_3038d_row6_col5, #T_3038d_row6_col6, #T_3038d_row6_col7, #T_3038d_row6_col8, #T_3038d_row6_col9, #T_3038d_row6_col10, #T_3038d_row6_col11, #T_3038d_row7_col0, #T_3038d_row7_col1, #T_3038d_row7_col2, #T_3038d_row7_col3, #T_3038d_row7_col4, #T_3038d_row7_col5, #T_3038d_row7_col6, #T_3038d_row7_col7, #T_3038d_row7_col8, #T_3038d_row7_col9, #T_3038d_row7_col10, #T_3038d_row7_col11, #T_3038d_row8_col0, #T_3038d_row8_col1, #T_3038d_row8_col2, #T_3038d_row8_col3, #T_3038d_row8_col4, #T_3038d_row8_col5, #T_3038d_row8_col6, #T_3038d_row8_col7, #T_3038d_row8_col8, #T_3038d_row8_col9, #T_3038d_row8_col10, #T_3038d_row8_col11, #T_3038d_row9_col0, #T_3038d_row9_col1, #T_3038d_row9_col2, #T_3038d_row9_col3, #T_3038d_row9_col4, #T_3038d_row9_col5, #T_3038d_row9_col6, #T_3038d_row9_col7, #T_3038d_row9_col8, #T_3038d_row9_col9, #T_3038d_row9_col10, #T_3038d_row9_col11 {\n",
       "  text-align: center;\n",
       "  border: 1.3px solid red;\n",
       "  color: blue;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_3038d\">\n",
       "  <caption>Table 1.2: Student Loan Table</caption>\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th id=\"T_3038d_level0_col0\" class=\"col_heading level0 col0\" >payment_history</th>\n",
       "      <th id=\"T_3038d_level0_col1\" class=\"col_heading level0 col1\" >location_parameter</th>\n",
       "      <th id=\"T_3038d_level0_col2\" class=\"col_heading level0 col2\" >stem_degree_score</th>\n",
       "      <th id=\"T_3038d_level0_col3\" class=\"col_heading level0 col3\" >gpa_ranking</th>\n",
       "      <th id=\"T_3038d_level0_col4\" class=\"col_heading level0 col4\" >alumni_success</th>\n",
       "      <th id=\"T_3038d_level0_col5\" class=\"col_heading level0 col5\" >study_major_code</th>\n",
       "      <th id=\"T_3038d_level0_col6\" class=\"col_heading level0 col6\" >time_to_completion</th>\n",
       "      <th id=\"T_3038d_level0_col7\" class=\"col_heading level0 col7\" >finance_workshop_score</th>\n",
       "      <th id=\"T_3038d_level0_col8\" class=\"col_heading level0 col8\" >cohort_ranking</th>\n",
       "      <th id=\"T_3038d_level0_col9\" class=\"col_heading level0 col9\" >total_loan_score</th>\n",
       "      <th id=\"T_3038d_level0_col10\" class=\"col_heading level0 col10\" >financial_aid_score</th>\n",
       "      <th id=\"T_3038d_level0_col11\" class=\"col_heading level0 col11\" >credit_ranking</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td id=\"T_3038d_row0_col0\" class=\"data row0 col0\" >7.40</td>\n",
       "      <td id=\"T_3038d_row0_col1\" class=\"data row0 col1\" >0.70</td>\n",
       "      <td id=\"T_3038d_row0_col2\" class=\"data row0 col2\" >0.00</td>\n",
       "      <td id=\"T_3038d_row0_col3\" class=\"data row0 col3\" >1.90</td>\n",
       "      <td id=\"T_3038d_row0_col4\" class=\"data row0 col4\" >0.08</td>\n",
       "      <td id=\"T_3038d_row0_col5\" class=\"data row0 col5\" >11.00</td>\n",
       "      <td id=\"T_3038d_row0_col6\" class=\"data row0 col6\" >34.00</td>\n",
       "      <td id=\"T_3038d_row0_col7\" class=\"data row0 col7\" >1.00</td>\n",
       "      <td id=\"T_3038d_row0_col8\" class=\"data row0 col8\" >3.51</td>\n",
       "      <td id=\"T_3038d_row0_col9\" class=\"data row0 col9\" >0.56</td>\n",
       "      <td id=\"T_3038d_row0_col10\" class=\"data row0 col10\" >9.40</td>\n",
       "      <td id=\"T_3038d_row0_col11\" class=\"data row0 col11\" >5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_3038d_row1_col0\" class=\"data row1 col0\" >7.80</td>\n",
       "      <td id=\"T_3038d_row1_col1\" class=\"data row1 col1\" >0.88</td>\n",
       "      <td id=\"T_3038d_row1_col2\" class=\"data row1 col2\" >0.00</td>\n",
       "      <td id=\"T_3038d_row1_col3\" class=\"data row1 col3\" >2.60</td>\n",
       "      <td id=\"T_3038d_row1_col4\" class=\"data row1 col4\" >0.10</td>\n",
       "      <td id=\"T_3038d_row1_col5\" class=\"data row1 col5\" >25.00</td>\n",
       "      <td id=\"T_3038d_row1_col6\" class=\"data row1 col6\" >67.00</td>\n",
       "      <td id=\"T_3038d_row1_col7\" class=\"data row1 col7\" >1.00</td>\n",
       "      <td id=\"T_3038d_row1_col8\" class=\"data row1 col8\" >3.20</td>\n",
       "      <td id=\"T_3038d_row1_col9\" class=\"data row1 col9\" >0.68</td>\n",
       "      <td id=\"T_3038d_row1_col10\" class=\"data row1 col10\" >9.80</td>\n",
       "      <td id=\"T_3038d_row1_col11\" class=\"data row1 col11\" >5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_3038d_row2_col0\" class=\"data row2 col0\" >7.80</td>\n",
       "      <td id=\"T_3038d_row2_col1\" class=\"data row2 col1\" >0.76</td>\n",
       "      <td id=\"T_3038d_row2_col2\" class=\"data row2 col2\" >0.04</td>\n",
       "      <td id=\"T_3038d_row2_col3\" class=\"data row2 col3\" >2.30</td>\n",
       "      <td id=\"T_3038d_row2_col4\" class=\"data row2 col4\" >0.09</td>\n",
       "      <td id=\"T_3038d_row2_col5\" class=\"data row2 col5\" >15.00</td>\n",
       "      <td id=\"T_3038d_row2_col6\" class=\"data row2 col6\" >54.00</td>\n",
       "      <td id=\"T_3038d_row2_col7\" class=\"data row2 col7\" >1.00</td>\n",
       "      <td id=\"T_3038d_row2_col8\" class=\"data row2 col8\" >3.26</td>\n",
       "      <td id=\"T_3038d_row2_col9\" class=\"data row2 col9\" >0.65</td>\n",
       "      <td id=\"T_3038d_row2_col10\" class=\"data row2 col10\" >9.80</td>\n",
       "      <td id=\"T_3038d_row2_col11\" class=\"data row2 col11\" >5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_3038d_row3_col0\" class=\"data row3 col0\" >11.20</td>\n",
       "      <td id=\"T_3038d_row3_col1\" class=\"data row3 col1\" >0.28</td>\n",
       "      <td id=\"T_3038d_row3_col2\" class=\"data row3 col2\" >0.56</td>\n",
       "      <td id=\"T_3038d_row3_col3\" class=\"data row3 col3\" >1.90</td>\n",
       "      <td id=\"T_3038d_row3_col4\" class=\"data row3 col4\" >0.07</td>\n",
       "      <td id=\"T_3038d_row3_col5\" class=\"data row3 col5\" >17.00</td>\n",
       "      <td id=\"T_3038d_row3_col6\" class=\"data row3 col6\" >60.00</td>\n",
       "      <td id=\"T_3038d_row3_col7\" class=\"data row3 col7\" >1.00</td>\n",
       "      <td id=\"T_3038d_row3_col8\" class=\"data row3 col8\" >3.16</td>\n",
       "      <td id=\"T_3038d_row3_col9\" class=\"data row3 col9\" >0.58</td>\n",
       "      <td id=\"T_3038d_row3_col10\" class=\"data row3 col10\" >9.80</td>\n",
       "      <td id=\"T_3038d_row3_col11\" class=\"data row3 col11\" >6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_3038d_row4_col0\" class=\"data row4 col0\" >7.40</td>\n",
       "      <td id=\"T_3038d_row4_col1\" class=\"data row4 col1\" >0.70</td>\n",
       "      <td id=\"T_3038d_row4_col2\" class=\"data row4 col2\" >0.00</td>\n",
       "      <td id=\"T_3038d_row4_col3\" class=\"data row4 col3\" >1.90</td>\n",
       "      <td id=\"T_3038d_row4_col4\" class=\"data row4 col4\" >0.08</td>\n",
       "      <td id=\"T_3038d_row4_col5\" class=\"data row4 col5\" >11.00</td>\n",
       "      <td id=\"T_3038d_row4_col6\" class=\"data row4 col6\" >34.00</td>\n",
       "      <td id=\"T_3038d_row4_col7\" class=\"data row4 col7\" >1.00</td>\n",
       "      <td id=\"T_3038d_row4_col8\" class=\"data row4 col8\" >3.51</td>\n",
       "      <td id=\"T_3038d_row4_col9\" class=\"data row4 col9\" >0.56</td>\n",
       "      <td id=\"T_3038d_row4_col10\" class=\"data row4 col10\" >9.40</td>\n",
       "      <td id=\"T_3038d_row4_col11\" class=\"data row4 col11\" >5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_3038d_row5_col0\" class=\"data row5 col0\" >7.40</td>\n",
       "      <td id=\"T_3038d_row5_col1\" class=\"data row5 col1\" >0.66</td>\n",
       "      <td id=\"T_3038d_row5_col2\" class=\"data row5 col2\" >0.00</td>\n",
       "      <td id=\"T_3038d_row5_col3\" class=\"data row5 col3\" >1.80</td>\n",
       "      <td id=\"T_3038d_row5_col4\" class=\"data row5 col4\" >0.07</td>\n",
       "      <td id=\"T_3038d_row5_col5\" class=\"data row5 col5\" >13.00</td>\n",
       "      <td id=\"T_3038d_row5_col6\" class=\"data row5 col6\" >40.00</td>\n",
       "      <td id=\"T_3038d_row5_col7\" class=\"data row5 col7\" >1.00</td>\n",
       "      <td id=\"T_3038d_row5_col8\" class=\"data row5 col8\" >3.51</td>\n",
       "      <td id=\"T_3038d_row5_col9\" class=\"data row5 col9\" >0.56</td>\n",
       "      <td id=\"T_3038d_row5_col10\" class=\"data row5 col10\" >9.40</td>\n",
       "      <td id=\"T_3038d_row5_col11\" class=\"data row5 col11\" >5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_3038d_row6_col0\" class=\"data row6 col0\" >7.90</td>\n",
       "      <td id=\"T_3038d_row6_col1\" class=\"data row6 col1\" >0.60</td>\n",
       "      <td id=\"T_3038d_row6_col2\" class=\"data row6 col2\" >0.06</td>\n",
       "      <td id=\"T_3038d_row6_col3\" class=\"data row6 col3\" >1.60</td>\n",
       "      <td id=\"T_3038d_row6_col4\" class=\"data row6 col4\" >0.07</td>\n",
       "      <td id=\"T_3038d_row6_col5\" class=\"data row6 col5\" >15.00</td>\n",
       "      <td id=\"T_3038d_row6_col6\" class=\"data row6 col6\" >59.00</td>\n",
       "      <td id=\"T_3038d_row6_col7\" class=\"data row6 col7\" >1.00</td>\n",
       "      <td id=\"T_3038d_row6_col8\" class=\"data row6 col8\" >3.30</td>\n",
       "      <td id=\"T_3038d_row6_col9\" class=\"data row6 col9\" >0.46</td>\n",
       "      <td id=\"T_3038d_row6_col10\" class=\"data row6 col10\" >9.40</td>\n",
       "      <td id=\"T_3038d_row6_col11\" class=\"data row6 col11\" >5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_3038d_row7_col0\" class=\"data row7 col0\" >7.30</td>\n",
       "      <td id=\"T_3038d_row7_col1\" class=\"data row7 col1\" >0.65</td>\n",
       "      <td id=\"T_3038d_row7_col2\" class=\"data row7 col2\" >0.00</td>\n",
       "      <td id=\"T_3038d_row7_col3\" class=\"data row7 col3\" >1.20</td>\n",
       "      <td id=\"T_3038d_row7_col4\" class=\"data row7 col4\" >0.07</td>\n",
       "      <td id=\"T_3038d_row7_col5\" class=\"data row7 col5\" >15.00</td>\n",
       "      <td id=\"T_3038d_row7_col6\" class=\"data row7 col6\" >21.00</td>\n",
       "      <td id=\"T_3038d_row7_col7\" class=\"data row7 col7\" >0.99</td>\n",
       "      <td id=\"T_3038d_row7_col8\" class=\"data row7 col8\" >3.39</td>\n",
       "      <td id=\"T_3038d_row7_col9\" class=\"data row7 col9\" >0.47</td>\n",
       "      <td id=\"T_3038d_row7_col10\" class=\"data row7 col10\" >10.00</td>\n",
       "      <td id=\"T_3038d_row7_col11\" class=\"data row7 col11\" >7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_3038d_row8_col0\" class=\"data row8 col0\" >7.80</td>\n",
       "      <td id=\"T_3038d_row8_col1\" class=\"data row8 col1\" >0.58</td>\n",
       "      <td id=\"T_3038d_row8_col2\" class=\"data row8 col2\" >0.02</td>\n",
       "      <td id=\"T_3038d_row8_col3\" class=\"data row8 col3\" >2.00</td>\n",
       "      <td id=\"T_3038d_row8_col4\" class=\"data row8 col4\" >0.07</td>\n",
       "      <td id=\"T_3038d_row8_col5\" class=\"data row8 col5\" >9.00</td>\n",
       "      <td id=\"T_3038d_row8_col6\" class=\"data row8 col6\" >18.00</td>\n",
       "      <td id=\"T_3038d_row8_col7\" class=\"data row8 col7\" >1.00</td>\n",
       "      <td id=\"T_3038d_row8_col8\" class=\"data row8 col8\" >3.36</td>\n",
       "      <td id=\"T_3038d_row8_col9\" class=\"data row8 col9\" >0.57</td>\n",
       "      <td id=\"T_3038d_row8_col10\" class=\"data row8 col10\" >9.50</td>\n",
       "      <td id=\"T_3038d_row8_col11\" class=\"data row8 col11\" >7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_3038d_row9_col0\" class=\"data row9 col0\" >7.50</td>\n",
       "      <td id=\"T_3038d_row9_col1\" class=\"data row9 col1\" >0.50</td>\n",
       "      <td id=\"T_3038d_row9_col2\" class=\"data row9 col2\" >0.36</td>\n",
       "      <td id=\"T_3038d_row9_col3\" class=\"data row9 col3\" >6.10</td>\n",
       "      <td id=\"T_3038d_row9_col4\" class=\"data row9 col4\" >0.07</td>\n",
       "      <td id=\"T_3038d_row9_col5\" class=\"data row9 col5\" >17.00</td>\n",
       "      <td id=\"T_3038d_row9_col6\" class=\"data row9 col6\" >102.00</td>\n",
       "      <td id=\"T_3038d_row9_col7\" class=\"data row9 col7\" >1.00</td>\n",
       "      <td id=\"T_3038d_row9_col8\" class=\"data row9 col8\" >3.35</td>\n",
       "      <td id=\"T_3038d_row9_col9\" class=\"data row9 col9\" >0.80</td>\n",
       "      <td id=\"T_3038d_row9_col10\" class=\"data row9 col10\" >10.50</td>\n",
       "      <td id=\"T_3038d_row9_col11\" class=\"data row9 col11\" >5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x16a024490>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pandas_processx.return_formatted_table(student_loan_dataframe, 'Table 1.2: Student Loan Table')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <br> **Section 2: Preprocessing**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2.1: Create the labels series (`y`)  from the “spam” column, and then create the features (`X`) DataFrame from the remaining columns.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Separate the Y Variable, the Labels**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_series = student_loan_dataframe['credit_ranking']\n",
    "\n",
    "logx.log_write_object(y_series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "credit_ranking\n",
       "5    681\n",
       "6    638\n",
       "7    199\n",
       "4     53\n",
       "8     18\n",
       "3     10\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_series.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Separate the X Variable, the Features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_dataframe = student_loan_dataframe.drop(columns = 'credit_ranking', axis = 1)\n",
    "\n",
    "logx.log_write_object(x_dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_a30ea caption {\n",
       "  color: black;\n",
       "  font-size: 20px;\n",
       "  font-style: bold;\n",
       "  text-align: center;\n",
       "}\n",
       "#T_a30ea_row0_col0, #T_a30ea_row0_col1, #T_a30ea_row0_col2, #T_a30ea_row0_col3, #T_a30ea_row0_col4, #T_a30ea_row0_col5, #T_a30ea_row0_col6, #T_a30ea_row0_col7, #T_a30ea_row0_col8, #T_a30ea_row0_col9, #T_a30ea_row0_col10, #T_a30ea_row1_col0, #T_a30ea_row1_col1, #T_a30ea_row1_col2, #T_a30ea_row1_col3, #T_a30ea_row1_col4, #T_a30ea_row1_col5, #T_a30ea_row1_col6, #T_a30ea_row1_col7, #T_a30ea_row1_col8, #T_a30ea_row1_col9, #T_a30ea_row1_col10, #T_a30ea_row2_col0, #T_a30ea_row2_col1, #T_a30ea_row2_col2, #T_a30ea_row2_col3, #T_a30ea_row2_col4, #T_a30ea_row2_col5, #T_a30ea_row2_col6, #T_a30ea_row2_col7, #T_a30ea_row2_col8, #T_a30ea_row2_col9, #T_a30ea_row2_col10, #T_a30ea_row3_col0, #T_a30ea_row3_col1, #T_a30ea_row3_col2, #T_a30ea_row3_col3, #T_a30ea_row3_col4, #T_a30ea_row3_col5, #T_a30ea_row3_col6, #T_a30ea_row3_col7, #T_a30ea_row3_col8, #T_a30ea_row3_col9, #T_a30ea_row3_col10, #T_a30ea_row4_col0, #T_a30ea_row4_col1, #T_a30ea_row4_col2, #T_a30ea_row4_col3, #T_a30ea_row4_col4, #T_a30ea_row4_col5, #T_a30ea_row4_col6, #T_a30ea_row4_col7, #T_a30ea_row4_col8, #T_a30ea_row4_col9, #T_a30ea_row4_col10, #T_a30ea_row5_col0, #T_a30ea_row5_col1, #T_a30ea_row5_col2, #T_a30ea_row5_col3, #T_a30ea_row5_col4, #T_a30ea_row5_col5, #T_a30ea_row5_col6, #T_a30ea_row5_col7, #T_a30ea_row5_col8, #T_a30ea_row5_col9, #T_a30ea_row5_col10, #T_a30ea_row6_col0, #T_a30ea_row6_col1, #T_a30ea_row6_col2, #T_a30ea_row6_col3, #T_a30ea_row6_col4, #T_a30ea_row6_col5, #T_a30ea_row6_col6, #T_a30ea_row6_col7, #T_a30ea_row6_col8, #T_a30ea_row6_col9, #T_a30ea_row6_col10, #T_a30ea_row7_col0, #T_a30ea_row7_col1, #T_a30ea_row7_col2, #T_a30ea_row7_col3, #T_a30ea_row7_col4, #T_a30ea_row7_col5, #T_a30ea_row7_col6, #T_a30ea_row7_col7, #T_a30ea_row7_col8, #T_a30ea_row7_col9, #T_a30ea_row7_col10, #T_a30ea_row8_col0, #T_a30ea_row8_col1, #T_a30ea_row8_col2, #T_a30ea_row8_col3, #T_a30ea_row8_col4, #T_a30ea_row8_col5, #T_a30ea_row8_col6, #T_a30ea_row8_col7, #T_a30ea_row8_col8, #T_a30ea_row8_col9, #T_a30ea_row8_col10, #T_a30ea_row9_col0, #T_a30ea_row9_col1, #T_a30ea_row9_col2, #T_a30ea_row9_col3, #T_a30ea_row9_col4, #T_a30ea_row9_col5, #T_a30ea_row9_col6, #T_a30ea_row9_col7, #T_a30ea_row9_col8, #T_a30ea_row9_col9, #T_a30ea_row9_col10 {\n",
       "  text-align: center;\n",
       "  border: 1.3px solid red;\n",
       "  color: blue;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_a30ea\">\n",
       "  <caption>Table 2.1: Student Loan Features DataFrame</caption>\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th id=\"T_a30ea_level0_col0\" class=\"col_heading level0 col0\" >payment_history</th>\n",
       "      <th id=\"T_a30ea_level0_col1\" class=\"col_heading level0 col1\" >location_parameter</th>\n",
       "      <th id=\"T_a30ea_level0_col2\" class=\"col_heading level0 col2\" >stem_degree_score</th>\n",
       "      <th id=\"T_a30ea_level0_col3\" class=\"col_heading level0 col3\" >gpa_ranking</th>\n",
       "      <th id=\"T_a30ea_level0_col4\" class=\"col_heading level0 col4\" >alumni_success</th>\n",
       "      <th id=\"T_a30ea_level0_col5\" class=\"col_heading level0 col5\" >study_major_code</th>\n",
       "      <th id=\"T_a30ea_level0_col6\" class=\"col_heading level0 col6\" >time_to_completion</th>\n",
       "      <th id=\"T_a30ea_level0_col7\" class=\"col_heading level0 col7\" >finance_workshop_score</th>\n",
       "      <th id=\"T_a30ea_level0_col8\" class=\"col_heading level0 col8\" >cohort_ranking</th>\n",
       "      <th id=\"T_a30ea_level0_col9\" class=\"col_heading level0 col9\" >total_loan_score</th>\n",
       "      <th id=\"T_a30ea_level0_col10\" class=\"col_heading level0 col10\" >financial_aid_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td id=\"T_a30ea_row0_col0\" class=\"data row0 col0\" >7.40</td>\n",
       "      <td id=\"T_a30ea_row0_col1\" class=\"data row0 col1\" >0.70</td>\n",
       "      <td id=\"T_a30ea_row0_col2\" class=\"data row0 col2\" >0.00</td>\n",
       "      <td id=\"T_a30ea_row0_col3\" class=\"data row0 col3\" >1.90</td>\n",
       "      <td id=\"T_a30ea_row0_col4\" class=\"data row0 col4\" >0.08</td>\n",
       "      <td id=\"T_a30ea_row0_col5\" class=\"data row0 col5\" >11.00</td>\n",
       "      <td id=\"T_a30ea_row0_col6\" class=\"data row0 col6\" >34.00</td>\n",
       "      <td id=\"T_a30ea_row0_col7\" class=\"data row0 col7\" >1.00</td>\n",
       "      <td id=\"T_a30ea_row0_col8\" class=\"data row0 col8\" >3.51</td>\n",
       "      <td id=\"T_a30ea_row0_col9\" class=\"data row0 col9\" >0.56</td>\n",
       "      <td id=\"T_a30ea_row0_col10\" class=\"data row0 col10\" >9.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_a30ea_row1_col0\" class=\"data row1 col0\" >7.80</td>\n",
       "      <td id=\"T_a30ea_row1_col1\" class=\"data row1 col1\" >0.88</td>\n",
       "      <td id=\"T_a30ea_row1_col2\" class=\"data row1 col2\" >0.00</td>\n",
       "      <td id=\"T_a30ea_row1_col3\" class=\"data row1 col3\" >2.60</td>\n",
       "      <td id=\"T_a30ea_row1_col4\" class=\"data row1 col4\" >0.10</td>\n",
       "      <td id=\"T_a30ea_row1_col5\" class=\"data row1 col5\" >25.00</td>\n",
       "      <td id=\"T_a30ea_row1_col6\" class=\"data row1 col6\" >67.00</td>\n",
       "      <td id=\"T_a30ea_row1_col7\" class=\"data row1 col7\" >1.00</td>\n",
       "      <td id=\"T_a30ea_row1_col8\" class=\"data row1 col8\" >3.20</td>\n",
       "      <td id=\"T_a30ea_row1_col9\" class=\"data row1 col9\" >0.68</td>\n",
       "      <td id=\"T_a30ea_row1_col10\" class=\"data row1 col10\" >9.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_a30ea_row2_col0\" class=\"data row2 col0\" >7.80</td>\n",
       "      <td id=\"T_a30ea_row2_col1\" class=\"data row2 col1\" >0.76</td>\n",
       "      <td id=\"T_a30ea_row2_col2\" class=\"data row2 col2\" >0.04</td>\n",
       "      <td id=\"T_a30ea_row2_col3\" class=\"data row2 col3\" >2.30</td>\n",
       "      <td id=\"T_a30ea_row2_col4\" class=\"data row2 col4\" >0.09</td>\n",
       "      <td id=\"T_a30ea_row2_col5\" class=\"data row2 col5\" >15.00</td>\n",
       "      <td id=\"T_a30ea_row2_col6\" class=\"data row2 col6\" >54.00</td>\n",
       "      <td id=\"T_a30ea_row2_col7\" class=\"data row2 col7\" >1.00</td>\n",
       "      <td id=\"T_a30ea_row2_col8\" class=\"data row2 col8\" >3.26</td>\n",
       "      <td id=\"T_a30ea_row2_col9\" class=\"data row2 col9\" >0.65</td>\n",
       "      <td id=\"T_a30ea_row2_col10\" class=\"data row2 col10\" >9.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_a30ea_row3_col0\" class=\"data row3 col0\" >11.20</td>\n",
       "      <td id=\"T_a30ea_row3_col1\" class=\"data row3 col1\" >0.28</td>\n",
       "      <td id=\"T_a30ea_row3_col2\" class=\"data row3 col2\" >0.56</td>\n",
       "      <td id=\"T_a30ea_row3_col3\" class=\"data row3 col3\" >1.90</td>\n",
       "      <td id=\"T_a30ea_row3_col4\" class=\"data row3 col4\" >0.07</td>\n",
       "      <td id=\"T_a30ea_row3_col5\" class=\"data row3 col5\" >17.00</td>\n",
       "      <td id=\"T_a30ea_row3_col6\" class=\"data row3 col6\" >60.00</td>\n",
       "      <td id=\"T_a30ea_row3_col7\" class=\"data row3 col7\" >1.00</td>\n",
       "      <td id=\"T_a30ea_row3_col8\" class=\"data row3 col8\" >3.16</td>\n",
       "      <td id=\"T_a30ea_row3_col9\" class=\"data row3 col9\" >0.58</td>\n",
       "      <td id=\"T_a30ea_row3_col10\" class=\"data row3 col10\" >9.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_a30ea_row4_col0\" class=\"data row4 col0\" >7.40</td>\n",
       "      <td id=\"T_a30ea_row4_col1\" class=\"data row4 col1\" >0.70</td>\n",
       "      <td id=\"T_a30ea_row4_col2\" class=\"data row4 col2\" >0.00</td>\n",
       "      <td id=\"T_a30ea_row4_col3\" class=\"data row4 col3\" >1.90</td>\n",
       "      <td id=\"T_a30ea_row4_col4\" class=\"data row4 col4\" >0.08</td>\n",
       "      <td id=\"T_a30ea_row4_col5\" class=\"data row4 col5\" >11.00</td>\n",
       "      <td id=\"T_a30ea_row4_col6\" class=\"data row4 col6\" >34.00</td>\n",
       "      <td id=\"T_a30ea_row4_col7\" class=\"data row4 col7\" >1.00</td>\n",
       "      <td id=\"T_a30ea_row4_col8\" class=\"data row4 col8\" >3.51</td>\n",
       "      <td id=\"T_a30ea_row4_col9\" class=\"data row4 col9\" >0.56</td>\n",
       "      <td id=\"T_a30ea_row4_col10\" class=\"data row4 col10\" >9.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_a30ea_row5_col0\" class=\"data row5 col0\" >7.40</td>\n",
       "      <td id=\"T_a30ea_row5_col1\" class=\"data row5 col1\" >0.66</td>\n",
       "      <td id=\"T_a30ea_row5_col2\" class=\"data row5 col2\" >0.00</td>\n",
       "      <td id=\"T_a30ea_row5_col3\" class=\"data row5 col3\" >1.80</td>\n",
       "      <td id=\"T_a30ea_row5_col4\" class=\"data row5 col4\" >0.07</td>\n",
       "      <td id=\"T_a30ea_row5_col5\" class=\"data row5 col5\" >13.00</td>\n",
       "      <td id=\"T_a30ea_row5_col6\" class=\"data row5 col6\" >40.00</td>\n",
       "      <td id=\"T_a30ea_row5_col7\" class=\"data row5 col7\" >1.00</td>\n",
       "      <td id=\"T_a30ea_row5_col8\" class=\"data row5 col8\" >3.51</td>\n",
       "      <td id=\"T_a30ea_row5_col9\" class=\"data row5 col9\" >0.56</td>\n",
       "      <td id=\"T_a30ea_row5_col10\" class=\"data row5 col10\" >9.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_a30ea_row6_col0\" class=\"data row6 col0\" >7.90</td>\n",
       "      <td id=\"T_a30ea_row6_col1\" class=\"data row6 col1\" >0.60</td>\n",
       "      <td id=\"T_a30ea_row6_col2\" class=\"data row6 col2\" >0.06</td>\n",
       "      <td id=\"T_a30ea_row6_col3\" class=\"data row6 col3\" >1.60</td>\n",
       "      <td id=\"T_a30ea_row6_col4\" class=\"data row6 col4\" >0.07</td>\n",
       "      <td id=\"T_a30ea_row6_col5\" class=\"data row6 col5\" >15.00</td>\n",
       "      <td id=\"T_a30ea_row6_col6\" class=\"data row6 col6\" >59.00</td>\n",
       "      <td id=\"T_a30ea_row6_col7\" class=\"data row6 col7\" >1.00</td>\n",
       "      <td id=\"T_a30ea_row6_col8\" class=\"data row6 col8\" >3.30</td>\n",
       "      <td id=\"T_a30ea_row6_col9\" class=\"data row6 col9\" >0.46</td>\n",
       "      <td id=\"T_a30ea_row6_col10\" class=\"data row6 col10\" >9.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_a30ea_row7_col0\" class=\"data row7 col0\" >7.30</td>\n",
       "      <td id=\"T_a30ea_row7_col1\" class=\"data row7 col1\" >0.65</td>\n",
       "      <td id=\"T_a30ea_row7_col2\" class=\"data row7 col2\" >0.00</td>\n",
       "      <td id=\"T_a30ea_row7_col3\" class=\"data row7 col3\" >1.20</td>\n",
       "      <td id=\"T_a30ea_row7_col4\" class=\"data row7 col4\" >0.07</td>\n",
       "      <td id=\"T_a30ea_row7_col5\" class=\"data row7 col5\" >15.00</td>\n",
       "      <td id=\"T_a30ea_row7_col6\" class=\"data row7 col6\" >21.00</td>\n",
       "      <td id=\"T_a30ea_row7_col7\" class=\"data row7 col7\" >0.99</td>\n",
       "      <td id=\"T_a30ea_row7_col8\" class=\"data row7 col8\" >3.39</td>\n",
       "      <td id=\"T_a30ea_row7_col9\" class=\"data row7 col9\" >0.47</td>\n",
       "      <td id=\"T_a30ea_row7_col10\" class=\"data row7 col10\" >10.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_a30ea_row8_col0\" class=\"data row8 col0\" >7.80</td>\n",
       "      <td id=\"T_a30ea_row8_col1\" class=\"data row8 col1\" >0.58</td>\n",
       "      <td id=\"T_a30ea_row8_col2\" class=\"data row8 col2\" >0.02</td>\n",
       "      <td id=\"T_a30ea_row8_col3\" class=\"data row8 col3\" >2.00</td>\n",
       "      <td id=\"T_a30ea_row8_col4\" class=\"data row8 col4\" >0.07</td>\n",
       "      <td id=\"T_a30ea_row8_col5\" class=\"data row8 col5\" >9.00</td>\n",
       "      <td id=\"T_a30ea_row8_col6\" class=\"data row8 col6\" >18.00</td>\n",
       "      <td id=\"T_a30ea_row8_col7\" class=\"data row8 col7\" >1.00</td>\n",
       "      <td id=\"T_a30ea_row8_col8\" class=\"data row8 col8\" >3.36</td>\n",
       "      <td id=\"T_a30ea_row8_col9\" class=\"data row8 col9\" >0.57</td>\n",
       "      <td id=\"T_a30ea_row8_col10\" class=\"data row8 col10\" >9.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_a30ea_row9_col0\" class=\"data row9 col0\" >7.50</td>\n",
       "      <td id=\"T_a30ea_row9_col1\" class=\"data row9 col1\" >0.50</td>\n",
       "      <td id=\"T_a30ea_row9_col2\" class=\"data row9 col2\" >0.36</td>\n",
       "      <td id=\"T_a30ea_row9_col3\" class=\"data row9 col3\" >6.10</td>\n",
       "      <td id=\"T_a30ea_row9_col4\" class=\"data row9 col4\" >0.07</td>\n",
       "      <td id=\"T_a30ea_row9_col5\" class=\"data row9 col5\" >17.00</td>\n",
       "      <td id=\"T_a30ea_row9_col6\" class=\"data row9 col6\" >102.00</td>\n",
       "      <td id=\"T_a30ea_row9_col7\" class=\"data row9 col7\" >1.00</td>\n",
       "      <td id=\"T_a30ea_row9_col8\" class=\"data row9 col8\" >3.35</td>\n",
       "      <td id=\"T_a30ea_row9_col9\" class=\"data row9 col9\" >0.80</td>\n",
       "      <td id=\"T_a30ea_row9_col10\" class=\"data row9 col10\" >10.50</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x16a042d10>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pandas_processx.return_formatted_table(x_dataframe, 'Table 2.1: Student Loan Features DataFrame')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2.2: Split the Data into Training and Testing Datasets by Using `train_test_split`.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_dataframe, x_test_dataframe, \\\n",
    "y_train_series, y_test_series \\\n",
    "    = train_test_split \\\n",
    "        (x_dataframe, y_series, \n",
    "         random_state = student_loans_constants.CONSTANT_DL_RANDOM_STATE_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "logx.log_write_object(x_train_dataframe)\n",
    "\n",
    "logx.log_write_object(x_test_dataframe)\n",
    "\n",
    "logx.log_write_object(y_train_series)\n",
    "\n",
    "logx.log_write_object(y_test_series)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2.3: Use the StandardScaler to Scale the X Variables**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Create a StandardScaler Instance**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_standard_scalar = StandardScaler()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Fit the StandardScaler**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_standard_scalar = current_standard_scalar.fit(x_train_dataframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Scale the Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_scaled_nparray = x_standard_scalar.transform(x_train_dataframe)\n",
    "\n",
    "logx.log_write_object(x_train_scaled_nparray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_scaled_nparray = x_standard_scalar.transform(x_test_dataframe)\n",
    "\n",
    "logx.log_write_object(x_test_scaled_nparray)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <br> **Section 3: Compile, Train, Evaluate, and Export the Model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3.1: Compile Model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Model Definition**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mThe number of inputs (features) in the model is 11.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "number_input_features_integer = len(x_train_scaled_nparray[0])\n",
    "\n",
    "logx.print_and_log_text \\\n",
    "    ('\\033[1m'\n",
    "     + 'The number of inputs (features) in the model is {:,}.' \\\n",
    "         .format(number_input_features_integer) \n",
    "     + '\\033[0m')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mThe number of nodes in the first hidden layer is 8.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "activation_layer1_string = 'relu'\n",
    "\n",
    "hidden_nodes_layer1_integer = 8\n",
    "\n",
    "logx.print_and_log_text \\\n",
    "    ('\\033[1m'\n",
    "     + 'The number of nodes in the first hidden layer is {:,}.' \\\n",
    "         .format(hidden_nodes_layer1_integer) \n",
    "     + '\\033[0m')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mThe number of nodes in the second hidden layer is 4.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "activation_layer2_string = 'relu'\n",
    "\n",
    "hidden_nodes_layer2_integer = 4\n",
    "\n",
    "logx.print_and_log_text \\\n",
    "    ('\\033[1m'\n",
    "     + 'The number of nodes in the second hidden layer is {:,}.' \\\n",
    "         .format(hidden_nodes_layer2_integer) \n",
    "     + '\\033[0m')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mThe number of nodes in the output layer is 1.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "activation_output_layer_string = 'linear'\n",
    "\n",
    "output_layer_integer = 1\n",
    "\n",
    "logx.print_and_log_text \\\n",
    "    ('\\033[1m'\n",
    "     + 'The number of nodes in the output layer is {:,}.' \\\n",
    "         .format(output_layer_integer) \n",
    "     + '\\033[0m')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Instantiate the Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "neural_network_sequential_model = tf.keras.models.Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Layers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/keras/src/layers/core/dense.py:86: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "neural_network_sequential_model.add \\\n",
    "    (tf.keras.layers.Dense \\\n",
    "         (units = hidden_nodes_layer1_integer, \n",
    "          activation = activation_layer1_string, \n",
    "          input_dim = number_input_features_integer))\n",
    "\n",
    "neural_network_sequential_model.add \\\n",
    "    (tf.keras.layers.Dense \\\n",
    "         (units = hidden_nodes_layer2_integer, \n",
    "          activation = activation_layer2_string))\n",
    "\n",
    "neural_network_sequential_model.add \\\n",
    "    (tf.keras.layers.Dense \\\n",
    "         (units = output_layer_integer, \n",
    "          activation = activation_output_layer_string))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Model Summary**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">36</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │             <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m)              │            \u001b[38;5;34m96\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m)              │            \u001b[38;5;34m36\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │             \u001b[38;5;34m5\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">137</span> (548.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m137\u001b[0m (548.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">137</span> (548.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m137\u001b[0m (548.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "neural_network_sequential_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Compile**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "neural_network_sequential_model.compile(loss = 'mean_squared_error', optimizer = 'adam', metrics = ['mse'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3.2: Fit and Train Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 28.8864 - mse: 28.8864 - val_loss: 26.4748 - val_mse: 26.4748\n",
      "Epoch 2/1000\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 25.0582 - mse: 25.0582 - val_loss: 21.7818 - val_mse: 21.7818\n",
      "Epoch 3/1000\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 21.1613 - mse: 21.1613 - val_loss: 16.4217 - val_mse: 16.4217\n",
      "Epoch 4/1000\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 15.5242 - mse: 15.5242 - val_loss: 11.2381 - val_mse: 11.2381\n",
      "Epoch 5/1000\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 10.0926 - mse: 10.0926 - val_loss: 7.5534 - val_mse: 7.5534\n",
      "Epoch 6/1000\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 6.9507 - mse: 6.9507 - val_loss: 5.5020 - val_mse: 5.5020\n",
      "Epoch 7/1000\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.7751 - mse: 4.7751 - val_loss: 4.3072 - val_mse: 4.3072\n",
      "Epoch 8/1000\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.7790 - mse: 3.7790 - val_loss: 3.4329 - val_mse: 3.4329\n",
      "Epoch 9/1000\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.9263 - mse: 2.9263 - val_loss: 2.8310 - val_mse: 2.8310\n",
      "Epoch 10/1000\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.1880 - mse: 2.1880 - val_loss: 2.4406 - val_mse: 2.4406\n",
      "Epoch 11/1000\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.1086 - mse: 2.1086 - val_loss: 2.1745 - val_mse: 2.1745\n",
      "Epoch 12/1000\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.8928 - mse: 1.8928 - val_loss: 2.0041 - val_mse: 2.0041\n",
      "Epoch 13/1000\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.8326 - mse: 1.8326 - val_loss: 1.8716 - val_mse: 1.8716\n",
      "Epoch 14/1000\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.6711 - mse: 1.6711 - val_loss: 1.7688 - val_mse: 1.7688\n",
      "Epoch 15/1000\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.6279 - mse: 1.6279 - val_loss: 1.6782 - val_mse: 1.6782\n",
      "Epoch 16/1000\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.5517 - mse: 1.5517 - val_loss: 1.5921 - val_mse: 1.5921\n",
      "Epoch 17/1000\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.4534 - mse: 1.4534 - val_loss: 1.5303 - val_mse: 1.5303\n",
      "Epoch 18/1000\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.3850 - mse: 1.3850 - val_loss: 1.4571 - val_mse: 1.4571\n",
      "Epoch 19/1000\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.3853 - mse: 1.3853 - val_loss: 1.4081 - val_mse: 1.4081\n",
      "Epoch 20/1000\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.3305 - mse: 1.3305 - val_loss: 1.3437 - val_mse: 1.3437\n",
      "Epoch 21/1000\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.2642 - mse: 1.2642 - val_loss: 1.2992 - val_mse: 1.2992\n",
      "Epoch 22/1000\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.2349 - mse: 1.2349 - val_loss: 1.2453 - val_mse: 1.2453\n",
      "Epoch 23/1000\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.1655 - mse: 1.1655 - val_loss: 1.2029 - val_mse: 1.2029\n",
      "Epoch 24/1000\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.1501 - mse: 1.1501 - val_loss: 1.1578 - val_mse: 1.1578\n",
      "Epoch 25/1000\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.1224 - mse: 1.1224 - val_loss: 1.1161 - val_mse: 1.1161\n",
      "Epoch 26/1000\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.0261 - mse: 1.0261 - val_loss: 1.0864 - val_mse: 1.0864\n",
      "Epoch 27/1000\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.9466 - mse: 0.9466 - val_loss: 1.0402 - val_mse: 1.0402\n",
      "Epoch 28/1000\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.0209 - mse: 1.0209 - val_loss: 1.0063 - val_mse: 1.0063\n",
      "Epoch 29/1000\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.0552 - mse: 1.0552 - val_loss: 0.9761 - val_mse: 0.9761\n",
      "Epoch 30/1000\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.9277 - mse: 0.9277 - val_loss: 0.9436 - val_mse: 0.9436\n",
      "Epoch 31/1000\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.8444 - mse: 0.8444 - val_loss: 0.9240 - val_mse: 0.9240\n",
      "Epoch 32/1000\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.8750 - mse: 0.8750 - val_loss: 0.8915 - val_mse: 0.8915\n",
      "Epoch 33/1000\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.8737 - mse: 0.8737 - val_loss: 0.8689 - val_mse: 0.8689\n",
      "Epoch 34/1000\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.8681 - mse: 0.8681 - val_loss: 0.8480 - val_mse: 0.8480\n",
      "Epoch 35/1000\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.8250 - mse: 0.8250 - val_loss: 0.8220 - val_mse: 0.8220\n",
      "Epoch 36/1000\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.8061 - mse: 0.8061 - val_loss: 0.8034 - val_mse: 0.8034\n",
      "Epoch 37/1000\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.7586 - mse: 0.7586 - val_loss: 0.7878 - val_mse: 0.7878\n",
      "Epoch 38/1000\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.7787 - mse: 0.7787 - val_loss: 0.7709 - val_mse: 0.7709\n",
      "Epoch 39/1000\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.6921 - mse: 0.6921 - val_loss: 0.7480 - val_mse: 0.7480\n",
      "Epoch 40/1000\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.7063 - mse: 0.7063 - val_loss: 0.7254 - val_mse: 0.7254\n",
      "Epoch 41/1000\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.7313 - mse: 0.7313 - val_loss: 0.7092 - val_mse: 0.7092\n",
      "Epoch 42/1000\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.6954 - mse: 0.6954 - val_loss: 0.6988 - val_mse: 0.6988\n",
      "Epoch 43/1000\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.7365 - mse: 0.7365 - val_loss: 0.6811 - val_mse: 0.6811\n",
      "Epoch 44/1000\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.6177 - mse: 0.6177 - val_loss: 0.6666 - val_mse: 0.6666\n",
      "Epoch 45/1000\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.6518 - mse: 0.6518 - val_loss: 0.6536 - val_mse: 0.6536\n",
      "Epoch 46/1000\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.6289 - mse: 0.6289 - val_loss: 0.6393 - val_mse: 0.6393\n",
      "Epoch 47/1000\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.6297 - mse: 0.6297 - val_loss: 0.6272 - val_mse: 0.6272\n",
      "Epoch 48/1000\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.6482 - mse: 0.6482 - val_loss: 0.6151 - val_mse: 0.6151\n",
      "Epoch 49/1000\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.6077 - mse: 0.6077 - val_loss: 0.6050 - val_mse: 0.6050\n",
      "Epoch 50/1000\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.5710 - mse: 0.5710 - val_loss: 0.5915 - val_mse: 0.5915\n",
      "Epoch 51/1000\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.5777 - mse: 0.5777 - val_loss: 0.5844 - val_mse: 0.5844\n",
      "Epoch 52/1000\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.5665 - mse: 0.5665 - val_loss: 0.5725 - val_mse: 0.5725\n",
      "Epoch 53/1000\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.5734 - mse: 0.5734 - val_loss: 0.5639 - val_mse: 0.5639\n",
      "Epoch 54/1000\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.5566 - mse: 0.5566 - val_loss: 0.5550 - val_mse: 0.5550\n",
      "Epoch 55/1000\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.5160 - mse: 0.5160 - val_loss: 0.5467 - val_mse: 0.5467\n",
      "Epoch 56/1000\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.5376 - mse: 0.5376 - val_loss: 0.5440 - val_mse: 0.5440\n",
      "Epoch 57/1000\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.5312 - mse: 0.5312 - val_loss: 0.5349 - val_mse: 0.5349\n",
      "Epoch 58/1000\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.5586 - mse: 0.5586 - val_loss: 0.5280 - val_mse: 0.5280\n",
      "Epoch 59/1000\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.4943 - mse: 0.4943 - val_loss: 0.5257 - val_mse: 0.5257\n",
      "Epoch 60/1000\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.5089 - mse: 0.5089 - val_loss: 0.5218 - val_mse: 0.5218\n",
      "Epoch 61/1000\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.5054 - mse: 0.5054 - val_loss: 0.5200 - val_mse: 0.5200\n",
      "Epoch 62/1000\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.4936 - mse: 0.4936 - val_loss: 0.5112 - val_mse: 0.5112\n",
      "Epoch 63/1000\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.5003 - mse: 0.5003 - val_loss: 0.5054 - val_mse: 0.5054\n",
      "Epoch 64/1000\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5173 - mse: 0.5173 - val_loss: 0.5062 - val_mse: 0.5062\n",
      "Epoch 65/1000\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.4603 - mse: 0.4603 - val_loss: 0.4987 - val_mse: 0.4987\n",
      "Epoch 66/1000\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.4464 - mse: 0.4464 - val_loss: 0.4954 - val_mse: 0.4954\n",
      "Epoch 67/1000\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.4488 - mse: 0.4488 - val_loss: 0.4956 - val_mse: 0.4956\n",
      "Epoch 68/1000\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.4760 - mse: 0.4760 - val_loss: 0.4924 - val_mse: 0.4924\n",
      "Epoch 69/1000\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.4801 - mse: 0.4801 - val_loss: 0.4891 - val_mse: 0.4891\n",
      "Epoch 70/1000\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.4583 - mse: 0.4583 - val_loss: 0.4866 - val_mse: 0.4866\n",
      "Epoch 71/1000\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.4300 - mse: 0.4300 - val_loss: 0.4858 - val_mse: 0.4858\n",
      "Epoch 72/1000\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.4680 - mse: 0.4680 - val_loss: 0.4858 - val_mse: 0.4858\n",
      "Epoch 73/1000\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.4541 - mse: 0.4541 - val_loss: 0.4804 - val_mse: 0.4804\n",
      "Epoch 74/1000\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.4704 - mse: 0.4704 - val_loss: 0.4809 - val_mse: 0.4809\n",
      "Epoch 75/1000\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.4886 - mse: 0.4886 - val_loss: 0.4795 - val_mse: 0.4795\n",
      "Epoch 76/1000\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.4145 - mse: 0.4145 - val_loss: 0.4775 - val_mse: 0.4775\n",
      "Epoch 77/1000\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.4219 - mse: 0.4219 - val_loss: 0.4752 - val_mse: 0.4752\n",
      "Epoch 78/1000\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.4596 - mse: 0.4596 - val_loss: 0.4776 - val_mse: 0.4776\n",
      "Epoch 79/1000\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.4317 - mse: 0.4317 - val_loss: 0.4767 - val_mse: 0.4767\n",
      "Epoch 80/1000\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.4223 - mse: 0.4223 - val_loss: 0.4763 - val_mse: 0.4763\n",
      "Epoch 81/1000\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.4042 - mse: 0.4042 - val_loss: 0.4720 - val_mse: 0.4720\n",
      "Epoch 82/1000\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.4289 - mse: 0.4289 - val_loss: 0.4714 - val_mse: 0.4714\n",
      "Epoch 83/1000\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.4234 - mse: 0.4234 - val_loss: 0.4766 - val_mse: 0.4766\n",
      "Epoch 84/1000\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4335 - mse: 0.4335 - val_loss: 0.4716 - val_mse: 0.4716\n",
      "Epoch 85/1000\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.4261 - mse: 0.4261 - val_loss: 0.4693 - val_mse: 0.4693\n",
      "Epoch 86/1000\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.4583 - mse: 0.4583 - val_loss: 0.4685 - val_mse: 0.4685\n",
      "Epoch 87/1000\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.4398 - mse: 0.4398 - val_loss: 0.4726 - val_mse: 0.4726\n",
      "Epoch 88/1000\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.4415 - mse: 0.4415 - val_loss: 0.4670 - val_mse: 0.4670\n",
      "Epoch 89/1000\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.4640 - mse: 0.4640 - val_loss: 0.4691 - val_mse: 0.4691\n",
      "Epoch 90/1000\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.4202 - mse: 0.4202 - val_loss: 0.4678 - val_mse: 0.4678\n",
      "Epoch 91/1000\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.4130 - mse: 0.4130 - val_loss: 0.4708 - val_mse: 0.4708\n",
      "Epoch 92/1000\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.4756 - mse: 0.4756 - val_loss: 0.4700 - val_mse: 0.4700\n",
      "Epoch 93/1000\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.4319 - mse: 0.4319 - val_loss: 0.4663 - val_mse: 0.4663\n",
      "Epoch 94/1000\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.4526 - mse: 0.4526 - val_loss: 0.4681 - val_mse: 0.4681\n",
      "Epoch 95/1000\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.4170 - mse: 0.4170 - val_loss: 0.4661 - val_mse: 0.4661\n",
      "Epoch 96/1000\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.3981 - mse: 0.3981 - val_loss: 0.4662 - val_mse: 0.4662\n",
      "Epoch 97/1000\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.3843 - mse: 0.3843 - val_loss: 0.4672 - val_mse: 0.4672\n",
      "Epoch 98/1000\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.4432 - mse: 0.4432 - val_loss: 0.4661 - val_mse: 0.4661\n",
      "Epoch 99/1000\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4418 - mse: 0.4418 - val_loss: 0.4657 - val_mse: 0.4657\n",
      "Epoch 100/1000\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.4256 - mse: 0.4256 - val_loss: 0.4667 - val_mse: 0.4667\n",
      "Epoch 101/1000\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.3988 - mse: 0.3988 - val_loss: 0.4681 - val_mse: 0.4681\n",
      "Epoch 102/1000\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.4270 - mse: 0.4270 - val_loss: 0.4687 - val_mse: 0.4687\n",
      "Epoch 103/1000\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.4402 - mse: 0.4402 - val_loss: 0.4662 - val_mse: 0.4662\n",
      "Epoch 104/1000\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.4204 - mse: 0.4204 - val_loss: 0.4662 - val_mse: 0.4662\n",
      "Epoch 105/1000\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.4319 - mse: 0.4319 - val_loss: 0.4632 - val_mse: 0.4632\n",
      "Epoch 106/1000\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.3983 - mse: 0.3983 - val_loss: 0.4630 - val_mse: 0.4630\n",
      "Epoch 107/1000\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.4189 - mse: 0.4189 - val_loss: 0.4667 - val_mse: 0.4667\n",
      "Epoch 108/1000\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.4038 - mse: 0.4038 - val_loss: 0.4647 - val_mse: 0.4647\n",
      "Epoch 109/1000\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.4095 - mse: 0.4095 - val_loss: 0.4633 - val_mse: 0.4633\n",
      "Epoch 110/1000\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.4139 - mse: 0.4139 - val_loss: 0.4649 - val_mse: 0.4649\n",
      "Epoch 111/1000\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.3929 - mse: 0.3929 - val_loss: 0.4638 - val_mse: 0.4638\n",
      "Epoch 112/1000\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.3748 - mse: 0.3748 - val_loss: 0.4623 - val_mse: 0.4623\n",
      "Epoch 113/1000\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.4134 - mse: 0.4134 - val_loss: 0.4671 - val_mse: 0.4671\n",
      "Epoch 114/1000\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.3782 - mse: 0.3782 - val_loss: 0.4607 - val_mse: 0.4607\n",
      "Epoch 115/1000\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.4130 - mse: 0.4130 - val_loss: 0.4623 - val_mse: 0.4623\n",
      "Epoch 116/1000\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.4362 - mse: 0.4362 - val_loss: 0.4610 - val_mse: 0.4610\n",
      "Epoch 117/1000\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.3992 - mse: 0.3992 - val_loss: 0.4630 - val_mse: 0.4630\n",
      "Epoch 118/1000\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.4017 - mse: 0.4017 - val_loss: 0.4602 - val_mse: 0.4602\n",
      "Epoch 119/1000\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.3918 - mse: 0.3918 - val_loss: 0.4632 - val_mse: 0.4632\n",
      "Epoch 120/1000\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.4079 - mse: 0.4079 - val_loss: 0.4625 - val_mse: 0.4625\n",
      "Epoch 121/1000\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.3939 - mse: 0.3939 - val_loss: 0.4598 - val_mse: 0.4598\n",
      "Epoch 122/1000\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.4038 - mse: 0.4038 - val_loss: 0.4576 - val_mse: 0.4576\n",
      "Epoch 123/1000\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.3894 - mse: 0.3894 - val_loss: 0.4604 - val_mse: 0.4604\n",
      "Epoch 124/1000\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.4057 - mse: 0.4057 - val_loss: 0.4575 - val_mse: 0.4575\n",
      "Epoch 125/1000\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.4169 - mse: 0.4169 - val_loss: 0.4601 - val_mse: 0.4601\n",
      "Epoch 126/1000\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.3830 - mse: 0.3830 - val_loss: 0.4601 - val_mse: 0.4601\n",
      "Epoch 127/1000\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.3858 - mse: 0.3858 - val_loss: 0.4588 - val_mse: 0.4588\n",
      "Epoch 128/1000\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.4062 - mse: 0.4062 - val_loss: 0.4602 - val_mse: 0.4602\n",
      "Epoch 129/1000\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.3690 - mse: 0.3690 - val_loss: 0.4588 - val_mse: 0.4588\n",
      "Epoch 130/1000\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.3827 - mse: 0.3827 - val_loss: 0.4593 - val_mse: 0.4593\n",
      "Epoch 131/1000\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.3906 - mse: 0.3906 - val_loss: 0.4567 - val_mse: 0.4567\n",
      "Epoch 132/1000\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.4421 - mse: 0.4421 - val_loss: 0.4615 - val_mse: 0.4615\n",
      "Epoch 133/1000\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.3848 - mse: 0.3848 - val_loss: 0.4551 - val_mse: 0.4551\n",
      "Epoch 134/1000\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.3686 - mse: 0.3686 - val_loss: 0.4589 - val_mse: 0.4589\n",
      "Epoch 135/1000\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.4331 - mse: 0.4331 - val_loss: 0.4562 - val_mse: 0.4562\n",
      "Epoch 136/1000\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.3686 - mse: 0.3686 - val_loss: 0.4604 - val_mse: 0.4604\n",
      "Epoch 137/1000\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.3740 - mse: 0.3740 - val_loss: 0.4574 - val_mse: 0.4574\n",
      "Epoch 138/1000\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4194 - mse: 0.4194 - val_loss: 0.4611 - val_mse: 0.4611\n",
      "Epoch 139/1000\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.3971 - mse: 0.3971 - val_loss: 0.4575 - val_mse: 0.4575\n",
      "Epoch 140/1000\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.3909 - mse: 0.3909 - val_loss: 0.4568 - val_mse: 0.4568\n",
      "Epoch 141/1000\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.3779 - mse: 0.3779 - val_loss: 0.4554 - val_mse: 0.4554\n",
      "Epoch 142/1000\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.3965 - mse: 0.3965 - val_loss: 0.4579 - val_mse: 0.4579\n",
      "Epoch 143/1000\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.3778 - mse: 0.3778 - val_loss: 0.4579 - val_mse: 0.4579\n",
      "Epoch 144/1000\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.3919 - mse: 0.3919 - val_loss: 0.4611 - val_mse: 0.4611\n",
      "Epoch 145/1000\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.3785 - mse: 0.3785 - val_loss: 0.4566 - val_mse: 0.4566\n",
      "Epoch 146/1000\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.3911 - mse: 0.3911 - val_loss: 0.4532 - val_mse: 0.4532\n",
      "Epoch 147/1000\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.3957 - mse: 0.3957 - val_loss: 0.4598 - val_mse: 0.4598\n",
      "Epoch 148/1000\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.3848 - mse: 0.3848 - val_loss: 0.4587 - val_mse: 0.4587\n",
      "Epoch 149/1000\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3668 - mse: 0.3668 - val_loss: 0.4563 - val_mse: 0.4563\n",
      "Epoch 150/1000\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.3803 - mse: 0.3803 - val_loss: 0.4481 - val_mse: 0.4481\n",
      "Epoch 151/1000\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.3820 - mse: 0.3820 - val_loss: 0.4534 - val_mse: 0.4534\n",
      "Epoch 152/1000\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.3645 - mse: 0.3645 - val_loss: 0.4558 - val_mse: 0.4558\n",
      "Epoch 153/1000\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.3817 - mse: 0.3817 - val_loss: 0.4533 - val_mse: 0.4533\n",
      "Epoch 154/1000\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.4308 - mse: 0.4308 - val_loss: 0.4511 - val_mse: 0.4511\n",
      "Epoch 155/1000\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.4260 - mse: 0.4260 - val_loss: 0.4518 - val_mse: 0.4518\n",
      "Epoch 156/1000\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.3944 - mse: 0.3944 - val_loss: 0.4565 - val_mse: 0.4565\n",
      "Epoch 157/1000\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.3949 - mse: 0.3949 - val_loss: 0.4500 - val_mse: 0.4500\n",
      "Epoch 158/1000\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.3809 - mse: 0.3809 - val_loss: 0.4549 - val_mse: 0.4549\n",
      "Epoch 159/1000\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.3947 - mse: 0.3947 - val_loss: 0.4491 - val_mse: 0.4491\n",
      "Epoch 160/1000\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.3807 - mse: 0.3807 - val_loss: 0.4507 - val_mse: 0.4507\n",
      "Epoch 161/1000\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.3791 - mse: 0.3791 - val_loss: 0.4596 - val_mse: 0.4596\n",
      "Epoch 162/1000\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.3931 - mse: 0.3931 - val_loss: 0.4474 - val_mse: 0.4474\n",
      "Epoch 163/1000\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.3887 - mse: 0.3887 - val_loss: 0.4515 - val_mse: 0.4515\n",
      "Epoch 164/1000\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.3943 - mse: 0.3943 - val_loss: 0.4489 - val_mse: 0.4489\n",
      "Epoch 165/1000\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.3955 - mse: 0.3955 - val_loss: 0.4499 - val_mse: 0.4499\n",
      "Epoch 166/1000\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.3673 - mse: 0.3673 - val_loss: 0.4486 - val_mse: 0.4486\n",
      "Epoch 167/1000\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.3574 - mse: 0.3574 - val_loss: 0.4475 - val_mse: 0.4475\n",
      "Epoch 168/1000\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.3581 - mse: 0.3581 - val_loss: 0.4534 - val_mse: 0.4534\n",
      "Epoch 169/1000\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.4017 - mse: 0.4017 - val_loss: 0.4473 - val_mse: 0.4473\n",
      "Epoch 170/1000\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.3835 - mse: 0.3835 - val_loss: 0.4476 - val_mse: 0.4476\n",
      "Epoch 171/1000\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.3633 - mse: 0.3633 - val_loss: 0.4485 - val_mse: 0.4485\n",
      "Epoch 172/1000\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 997us/step - loss: 0.3990 - mse: 0.3990 - val_loss: 0.4480 - val_mse: 0.4480\n",
      "Epoch 173/1000\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.3737 - mse: 0.3737 - val_loss: 0.4612 - val_mse: 0.4612\n",
      "Epoch 174/1000\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.3946 - mse: 0.3946 - val_loss: 0.4486 - val_mse: 0.4486\n",
      "Epoch 175/1000\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.3861 - mse: 0.3861 - val_loss: 0.4478 - val_mse: 0.4478\n",
      "Epoch 176/1000\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.4183 - mse: 0.4183 - val_loss: 0.4500 - val_mse: 0.4500\n",
      "Epoch 177/1000\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.3636 - mse: 0.3636 - val_loss: 0.4506 - val_mse: 0.4506\n",
      "Epoch 178/1000\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.3681 - mse: 0.3681 - val_loss: 0.4563 - val_mse: 0.4563\n",
      "Epoch 179/1000\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.3678 - mse: 0.3678 - val_loss: 0.4486 - val_mse: 0.4486\n",
      "Epoch 180/1000\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.3859 - mse: 0.3859 - val_loss: 0.4500 - val_mse: 0.4500\n",
      "Epoch 181/1000\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.3969 - mse: 0.3969 - val_loss: 0.4503 - val_mse: 0.4503\n",
      "Epoch 182/1000\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3777 - mse: 0.3777 - val_loss: 0.4503 - val_mse: 0.4503\n",
      "Epoch 183/1000\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.3739 - mse: 0.3739 - val_loss: 0.4498 - val_mse: 0.4498\n",
      "Epoch 184/1000\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 993us/step - loss: 0.3665 - mse: 0.3665 - val_loss: 0.4502 - val_mse: 0.4502\n",
      "Epoch 185/1000\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 996us/step - loss: 0.3543 - mse: 0.3543 - val_loss: 0.4506 - val_mse: 0.4506\n",
      "Epoch 186/1000\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 984us/step - loss: 0.4009 - mse: 0.4009 - val_loss: 0.4476 - val_mse: 0.4476\n",
      "Epoch 187/1000\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.3758 - mse: 0.3758 - val_loss: 0.4516 - val_mse: 0.4516\n",
      "Epoch 188/1000\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.3820 - mse: 0.3820 - val_loss: 0.4508 - val_mse: 0.4508\n",
      "Epoch 189/1000\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.3875 - mse: 0.3875 - val_loss: 0.4523 - val_mse: 0.4523\n",
      "Epoch 190/1000\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.3773 - mse: 0.3773 - val_loss: 0.4502 - val_mse: 0.4502\n",
      "Epoch 191/1000\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.3591 - mse: 0.3591 - val_loss: 0.4481 - val_mse: 0.4481\n",
      "Epoch 192/1000\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.3620 - mse: 0.3620 - val_loss: 0.4463 - val_mse: 0.4463\n",
      "Epoch 193/1000\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.3732 - mse: 0.3732 - val_loss: 0.4504 - val_mse: 0.4504\n",
      "Epoch 194/1000\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.3798 - mse: 0.3798 - val_loss: 0.4473 - val_mse: 0.4473\n",
      "Epoch 195/1000\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.3966 - mse: 0.3966 - val_loss: 0.4531 - val_mse: 0.4531\n",
      "Epoch 196/1000\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.3912 - mse: 0.3912 - val_loss: 0.4495 - val_mse: 0.4495\n",
      "Epoch 197/1000\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.3874 - mse: 0.3874 - val_loss: 0.4501 - val_mse: 0.4501\n",
      "Epoch 198/1000\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.3846 - mse: 0.3846 - val_loss: 0.4449 - val_mse: 0.4449\n",
      "Epoch 199/1000\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.3730 - mse: 0.3730 - val_loss: 0.4468 - val_mse: 0.4468\n",
      "Epoch 200/1000\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.3972 - mse: 0.3972 - val_loss: 0.4525 - val_mse: 0.4525\n",
      "Epoch 201/1000\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 985us/step - loss: 0.3742 - mse: 0.3742 - val_loss: 0.4482 - val_mse: 0.4482\n",
      "Epoch 202/1000\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.3546 - mse: 0.3546 - val_loss: 0.4456 - val_mse: 0.4456\n",
      "Epoch 203/1000\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1000us/step - loss: 0.3781 - mse: 0.3781 - val_loss: 0.4466 - val_mse: 0.4466\n",
      "Epoch 204/1000\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.3500 - mse: 0.3500 - val_loss: 0.4439 - val_mse: 0.4439\n",
      "Epoch 205/1000\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3700 - mse: 0.3700 - val_loss: 0.4449 - val_mse: 0.4449\n",
      "Epoch 206/1000\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.3874 - mse: 0.3874 - val_loss: 0.4473 - val_mse: 0.4473\n",
      "Epoch 207/1000\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1000us/step - loss: 0.3644 - mse: 0.3644 - val_loss: 0.4487 - val_mse: 0.4487\n",
      "Epoch 208/1000\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.3763 - mse: 0.3763 - val_loss: 0.4476 - val_mse: 0.4476\n",
      "Epoch 209/1000\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 992us/step - loss: 0.3949 - mse: 0.3949 - val_loss: 0.4480 - val_mse: 0.4480\n",
      "Epoch 210/1000\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 999us/step - loss: 0.3955 - mse: 0.3955 - val_loss: 0.4469 - val_mse: 0.4469\n",
      "Epoch 211/1000\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.3612 - mse: 0.3612 - val_loss: 0.4435 - val_mse: 0.4435\n",
      "Epoch 212/1000\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.3964 - mse: 0.3964 - val_loss: 0.4454 - val_mse: 0.4454\n",
      "Epoch 213/1000\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.3637 - mse: 0.3637 - val_loss: 0.4473 - val_mse: 0.4473\n",
      "Epoch 214/1000\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 997us/step - loss: 0.3703 - mse: 0.3703 - val_loss: 0.4508 - val_mse: 0.4508\n",
      "Epoch 215/1000\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.3707 - mse: 0.3707 - val_loss: 0.4471 - val_mse: 0.4471\n",
      "Epoch 216/1000\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.3765 - mse: 0.3765 - val_loss: 0.4450 - val_mse: 0.4450\n",
      "Epoch 217/1000\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 983us/step - loss: 0.3580 - mse: 0.3580 - val_loss: 0.4471 - val_mse: 0.4471\n",
      "Epoch 218/1000\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 985us/step - loss: 0.3639 - mse: 0.3639 - val_loss: 0.4452 - val_mse: 0.4452\n",
      "Epoch 219/1000\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 991us/step - loss: 0.3996 - mse: 0.3996 - val_loss: 0.4475 - val_mse: 0.4475\n",
      "Epoch 220/1000\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.3860 - mse: 0.3860 - val_loss: 0.4463 - val_mse: 0.4463\n",
      "Epoch 221/1000\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.3702 - mse: 0.3702 - val_loss: 0.4439 - val_mse: 0.4439\n",
      "Epoch 222/1000\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.3673 - mse: 0.3673 - val_loss: 0.4437 - val_mse: 0.4437\n",
      "Epoch 223/1000\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 988us/step - loss: 0.3896 - mse: 0.3896 - val_loss: 0.4495 - val_mse: 0.4495\n",
      "Epoch 224/1000\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.3626 - mse: 0.3626 - val_loss: 0.4475 - val_mse: 0.4475\n",
      "Epoch 225/1000\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3523 - mse: 0.3523 - val_loss: 0.4460 - val_mse: 0.4460\n",
      "Epoch 226/1000\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.3687 - mse: 0.3687 - val_loss: 0.4469 - val_mse: 0.4469\n",
      "Epoch 227/1000\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.3700 - mse: 0.3700 - val_loss: 0.4464 - val_mse: 0.4464\n",
      "Epoch 228/1000\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 998us/step - loss: 0.3912 - mse: 0.3912 - val_loss: 0.4480 - val_mse: 0.4480\n",
      "Epoch 229/1000\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.3768 - mse: 0.3768 - val_loss: 0.4492 - val_mse: 0.4492\n",
      "Epoch 230/1000\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.3717 - mse: 0.3717 - val_loss: 0.4498 - val_mse: 0.4498\n",
      "Epoch 231/1000\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.3743 - mse: 0.3743 - val_loss: 0.4441 - val_mse: 0.4441\n",
      "Epoch 232/1000\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 990us/step - loss: 0.4041 - mse: 0.4041 - val_loss: 0.4490 - val_mse: 0.4490\n",
      "Epoch 233/1000\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 997us/step - loss: 0.3580 - mse: 0.3580 - val_loss: 0.4488 - val_mse: 0.4488\n",
      "Epoch 234/1000\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.3491 - mse: 0.3491 - val_loss: 0.4498 - val_mse: 0.4498\n",
      "Epoch 235/1000\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.3647 - mse: 0.3647 - val_loss: 0.4524 - val_mse: 0.4524\n",
      "Epoch 236/1000\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 994us/step - loss: 0.3403 - mse: 0.3403 - val_loss: 0.4512 - val_mse: 0.4512\n",
      "Epoch 237/1000\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.3588 - mse: 0.3588 - val_loss: 0.4521 - val_mse: 0.4521\n",
      "Epoch 238/1000\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 988us/step - loss: 0.3790 - mse: 0.3790 - val_loss: 0.4499 - val_mse: 0.4499\n",
      "Epoch 239/1000\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.3731 - mse: 0.3731 - val_loss: 0.4508 - val_mse: 0.4508\n",
      "Epoch 240/1000\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.3894 - mse: 0.3894 - val_loss: 0.4508 - val_mse: 0.4508\n",
      "Epoch 241/1000\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.3652 - mse: 0.3652 - val_loss: 0.4503 - val_mse: 0.4503\n",
      "Epoch 242/1000\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.3730 - mse: 0.3730 - val_loss: 0.4513 - val_mse: 0.4513\n",
      "Epoch 243/1000\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.3685 - mse: 0.3685 - val_loss: 0.4516 - val_mse: 0.4516\n",
      "Epoch 244/1000\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.3573 - mse: 0.3573 - val_loss: 0.4511 - val_mse: 0.4511\n",
      "Epoch 245/1000\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.3729 - mse: 0.3729 - val_loss: 0.4512 - val_mse: 0.4512\n",
      "Epoch 246/1000\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.3810 - mse: 0.3810 - val_loss: 0.4474 - val_mse: 0.4474\n",
      "Epoch 247/1000\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.3812 - mse: 0.3812 - val_loss: 0.4516 - val_mse: 0.4516\n",
      "Epoch 248/1000\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.3638 - mse: 0.3638 - val_loss: 0.4522 - val_mse: 0.4522\n",
      "Epoch 249/1000\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.3651 - mse: 0.3651 - val_loss: 0.4530 - val_mse: 0.4530\n",
      "Epoch 250/1000\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.3624 - mse: 0.3624 - val_loss: 0.4532 - val_mse: 0.4532\n",
      "Epoch 251/1000\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.3561 - mse: 0.3561 - val_loss: 0.4532 - val_mse: 0.4532\n",
      "Epoch 252/1000\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.3445 - mse: 0.3445 - val_loss: 0.4529 - val_mse: 0.4529\n",
      "Epoch 253/1000\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.3924 - mse: 0.3924 - val_loss: 0.4560 - val_mse: 0.4560\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x16a012850>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "earlystopping_callback \\\n",
    "    = tf.keras.callbacks.EarlyStopping  \\\n",
    "        (monitor = 'val_mse', mode = 'min', patience = 42, restore_best_weights = True)\n",
    "\n",
    "neural_network_sequential_model \\\n",
    "    .fit \\\n",
    "        (x_train_scaled_nparray,\n",
    "         y_train_series.values,\n",
    "         epochs = 1000,\n",
    "         validation_data = (x_test_scaled_nparray, y_test_series.values),\n",
    "         callbacks = [earlystopping_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3.3: Evaluate Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13/13 - 0s - 1ms/step - loss: 0.4435 - mse: 0.4435\n",
      "\n",
      "Model Loss: 44.35%, Model Accuracy: 44.35%\n"
     ]
    }
   ],
   "source": [
    "model_loss_float, model_mse_float \\\n",
    "    = neural_network_sequential_model.evaluate(x_test_scaled_nparray, y_test_series.values, verbose = 2)\n",
    "\n",
    "logx.print_and_log_text \\\n",
    "    (f'\\nModel Loss: {round(model_loss_float * 100, 2)}%, '\n",
    "     + f'Model Accuracy: {round(model_mse_float * 100, 2)}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3.4: Save and Export Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "neural_network_sequential_model.save(student_loans_constants.CONSTANT_NN_MODEL_FILE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <br> **Section 4: Predict Loan Repayment Success**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4.1: Reload Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "reloaded_neural_network_sequential_model \\\n",
    "    = tf.keras.models.load_model \\\n",
    "        (student_loans_constants.CONSTANT_NN_MODEL_FILE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4.2: Predictions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n"
     ]
    }
   ],
   "source": [
    "predictions_nparray \\\n",
    "    = reloaded_neural_network_sequential_model.predict \\\n",
    "        (x_test_scaled_nparray).round().astype('int32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4.3: Compare Predictions and Actual Values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison_dataframe \\\n",
    "    = pd.DataFrame({'predictions': predictions_nparray.ravel(), 'actual': y_test_series.values})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_31d24 caption {\n",
       "  color: black;\n",
       "  font-size: 20px;\n",
       "  font-style: bold;\n",
       "  text-align: center;\n",
       "}\n",
       "#T_31d24_row0_col0, #T_31d24_row0_col1, #T_31d24_row1_col0, #T_31d24_row1_col1, #T_31d24_row2_col0, #T_31d24_row2_col1, #T_31d24_row3_col0, #T_31d24_row3_col1, #T_31d24_row4_col0, #T_31d24_row4_col1, #T_31d24_row5_col0, #T_31d24_row5_col1, #T_31d24_row6_col0, #T_31d24_row6_col1, #T_31d24_row7_col0, #T_31d24_row7_col1, #T_31d24_row8_col0, #T_31d24_row8_col1, #T_31d24_row9_col0, #T_31d24_row9_col1 {\n",
       "  text-align: center;\n",
       "  border: 1.3px solid red;\n",
       "  color: blue;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_31d24\">\n",
       "  <caption>Table 4.3: Model Predictions vs. Actual Values</caption>\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th id=\"T_31d24_level0_col0\" class=\"col_heading level0 col0\" >predictions</th>\n",
       "      <th id=\"T_31d24_level0_col1\" class=\"col_heading level0 col1\" >actual</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td id=\"T_31d24_row0_col0\" class=\"data row0 col0\" >6</td>\n",
       "      <td id=\"T_31d24_row0_col1\" class=\"data row0 col1\" >6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_31d24_row1_col0\" class=\"data row1 col0\" >6</td>\n",
       "      <td id=\"T_31d24_row1_col1\" class=\"data row1 col1\" >6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_31d24_row2_col0\" class=\"data row2 col0\" >5</td>\n",
       "      <td id=\"T_31d24_row2_col1\" class=\"data row2 col1\" >5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_31d24_row3_col0\" class=\"data row3 col0\" >5</td>\n",
       "      <td id=\"T_31d24_row3_col1\" class=\"data row3 col1\" >6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_31d24_row4_col0\" class=\"data row4 col0\" >6</td>\n",
       "      <td id=\"T_31d24_row4_col1\" class=\"data row4 col1\" >6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_31d24_row5_col0\" class=\"data row5 col0\" >5</td>\n",
       "      <td id=\"T_31d24_row5_col1\" class=\"data row5 col1\" >7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_31d24_row6_col0\" class=\"data row6 col0\" >5</td>\n",
       "      <td id=\"T_31d24_row6_col1\" class=\"data row6 col1\" >4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_31d24_row7_col0\" class=\"data row7 col0\" >6</td>\n",
       "      <td id=\"T_31d24_row7_col1\" class=\"data row7 col1\" >5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_31d24_row8_col0\" class=\"data row8 col0\" >5</td>\n",
       "      <td id=\"T_31d24_row8_col1\" class=\"data row8 col1\" >5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_31d24_row9_col0\" class=\"data row9 col0\" >5</td>\n",
       "      <td id=\"T_31d24_row9_col1\" class=\"data row9 col1\" >5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x16aa93390>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pandas_processx.return_formatted_table \\\n",
    "    (comparison_dataframe, 'Table 4.3: Model Predictions vs. Actual Values')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logx.end_program()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
