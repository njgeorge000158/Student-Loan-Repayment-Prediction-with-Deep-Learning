# Spam-Detection-with-Supervised-Machine-Learning-Models

----

## Table of Contents (spam_detector_colab.ipynb)

----

# <br><br> **Section 1: Extraction and Transformation**
> ## <br> **1.1: Read the CSV data into a Pandas DataFrame**
> ## <br> **1.2: Display Spam DataFrame**
> ## <br> **1.3: Create the labels series (y) from the “spam” column, and then create the features (X) DataFrame from the remaining columns.**
>> ### **Separate the Y Variable, The Labels**
>> ### **Review the Y Series**
>> ### **Check the Balance of the Labels Variable (y) by Using the value_counts Function.**
>> ### **Separate the X Variable, the Features**
>> ### **Review the X DataFrame**
> ## <br> **1.4: Split the Data into Training and Testing Datasets by Using train_test_split.**
> ## <br> **1.5: Use the StandardScaler to Scale the X Variables**
>> ### **Scale Training and Test Data as Numpy Arrays**
>> ### **Create Scaled X Variable DataFrames**
>> ### **Display Scaled Training and Testing Data**
# <br><br> **Section 2: Undersampled and OverSampled Spam Data**
> ## <br> **2.1: Instantiate the Random Undersampler Instance**
> ## <br> **2.2: Instantiate the Random Oversampler Instance**
> ## <br> **2.4: Instantiate the SMOTE Instance**
> ## <br> **2.5: Instantiate the SMOTEENN Instance**
> ## <br> **2.6: Check the Balance of the Labels Variable (y) by Using the value_counts Function.**
> ## <br> **2.7: Display Normalized Resampled Training and Testing Data**
# <br><br> **Section 3: Logistic Regression Models**
> ## <br> **3.1: Fit Models by Using the Scaled Training Data**
>> ### **Original**
>> ### **Random Undersampling**
>> ### **Random Oversampling**
>> ### **Cluster Centroids**
>> ### **SMOTE**
>> ### **SMOTEEN**
> ## <br> **3.2: Display the Model Scores Using the Scaled Training and Testing Data**
>> ### **Original**
>> ### **Random Undersampling**
>> ### **Random Oversampling**
>> ### **Cluster Centroids**
>> ### **SMOTE**
>> ### **SMOTEEN**
> ## <br> **3.3: Calculate Training and Test Predictions**
>> ### **Original**
>> ### **Random Undersampling**
>> ### **Random Oversampling**
>> ### **Cluster Centroids**
>> ### **SMOTE**
>> ### **SMOTEEN**
# <br><br> **Section 4: Decision Tree Models**
> ## <br> **4.1: Fit Models by Using the Scaled Training Data**
>> ### **Original**
>> ### **Random Undersampling**
>> ### **Random Oversampling**
>> ### **Cluster Centroids**
>> ### **SMOTE**
>> ### **SMOTEEN**
> ## <br> **4.2: Display the Model Scores Using the Scaled Training and Testing Data**
>> ### **Original**
>> ### **Random Undersampling**
>> ### **Random Oversampling**
>> ### **Cluster Centroids**
>> ### **SMOTE**
>> ### **SMOTEEN**
> ## <br> **4.3: Calculate Training and Test Predictions**
>> ### **Original**
>> ### **Random Undersampling**
>> ### **Random Oversampling**
>> ### **Cluster Centroids**
>> ### **SMOTE**
>> ### **SMOTEEN**
# <br><br> **Section 5: Random Forest Models**
> ## <br> **5.1: Fit Models by Using the Scaled Training Data**
>> ### **Original**
>> ### **Random Undersampling**
>> ### **Random Oversampling**
>> ### **Cluster Centroids**
>> ### **SMOTE**
>> ### **SMOTEEN**
> ## <br> **5.2: Display the Model Scores Using the Scaled Training and Testing Data**
>> ### **Original**
>> ### **Random Undersampling**
>> ### **Random Oversampling**
>> ### **Cluster Centroids**
>> ### **SMOTE**
>> ### **SMOTEEN**
> ## <br> **5.3: Calculate Training and Test Predictions**
>> ### **Original**
>> ### **Random Undersampling**
>> ### **Random Oversampling**
>> ### **Cluster Centroids**
>> ### **SMOTE**
>> ### **SMOTEEN**
# <br><br> **Section 6: Support Vector Machine (SVM) Models**
> ## <br> **6.1: Fit Models by Using the Scaled Training Data**
>> ### **Original**
>> ### **Random Undersampling**
>> ### **Random Oversampling**
>> ### **Cluster Centroids**
>> ### **SMOTE**
>> ### **SMOTEEN**
> ## <br> **6.2: Display the Model Scores Using the Scaled Training and Testing Data**
>> ### **Original**
>> ### **Random Undersampling**
>> ### **Random Oversampling**
>> ### **Cluster Centroids**
>> ### **SMOTE**
>> ### **SMOTEEN**
> ## <br> **6.3: Calculate Training and Test Predictions**
>> ### **Original**
>> ### **Random Undersampling**
>> ### **Random Oversampling**
>> ### **Cluster Centroids**
>> ### **SMOTE**
>> ### **SMOTEEN**
# <br><br> **Section 7: K-Nearest Neighbor (KNN) Models**
> ## <br> **7.1: Fit Models by Using the Scaled Training Data**
>> ### **Original**
>> ### **Random Undersampling**
>> ### **Random Oversampling**
>> ### **Cluster Centroids**
>> ### **SMOTE**
>> ### **SMOTEEN**
> ## <br> **7.2: Display the Model Scores Using the Scaled Training and Testing Data**
>> ### **Original**
>> ### **Random Undersampling**
>> ### **Random Oversampling**
>> ### **Cluster Centroids**
>> ### **SMOTE**
>> ### **SMOTEEN**
> ## <br> **7.3: Calculate Training and Test Predictions**
>> ### **Original**
>> ### **Random Undersampling**
>> ### **Random Oversampling**
>> ### **Cluster Centroids**
>> ### **SMOTE**
>> ### **SMOTEEN**
# <br><br> **Section 8: Gaussian Naive Bayes (GNB) Models**
> ## <br> **8.1: Fit Models by Using the Scaled Training Data**
>> ### **Original**
>> ### **Random Undersampling**
>> ### **Random Oversampling**
>> ### **Cluster Centroids**
>> ### **SMOTE**
>> ### **SMOTEEN**
> ## <br> **8.2: Display the Model Scores Using the Scaled Training and Testing Data**
>> ### **Original**
>> ### **Random Undersampling**
>> ### **Random Oversampling**
>> ### **Cluster Centroids**
>> ### **SMOTE**
>> ### **SMOTEEN**
> ## <br> **8.3: Calculate Training and Test Predictions**
>> ### **Original**
>> ### **Random Undersampling**
>> ### **Random Oversampling**
>> ### **Cluster Centroids**
>> ### **SMOTE**
>> ### **SMOTEEN**
# <br><br> **Section 9: Evaluate Model Performance**
> ## <br> **9.1: Logistic Regression**
>> ### **Original**
>> ### **Random Undersampling**
>> ### **Random Oversampling**
>> ### **Cluster Centroids**
>> ### **SMOTE**
>> ### **SMOTEEN**
> ## <br> **9.2: Decision Tree**
>> ### **Original**
>> ### **Random Undersampling**
>> ### **Random Oversampling**
>> ### **Cluster Centroids**
>> ### **SMOTE**
>> ### **SMOTEEN**
> ## <br> **9.3: Random Forest**
>> ### **Original**
>> ### **Random Undersampling**
>> ### **Random Oversampling**
>> ### **Cluster Centroids**
>> ### **SMOTE**
>> ### **SMOTEEN**
> ## <br> **9.4: Support Vector Machine (SVM)**
>> ### **Original**
>> ### **Random Undersampling**
>> ### **Random Oversampling**
>> ### **Cluster Centroids**
>> ### **SMOTE**
>> ### **SMOTEEN**
> ## <br> **9.5: K-Nearest Neighbor (KNN)**
>> ### **Original**
>> ### **Random Undersampling**
>> ### **Random Oversampling**
>> ### **Cluster Centroids**
>> ### **SMOTE**
>> ### **SMOTEEN**
> ## <br> **9.6: Gaussian Naive Bayes (GNB)**
>> ### **Original**
>> ### **Random Undersampling**
>> ### **Random Oversampling**
>> ### **Cluster Centroids**
>> ### **SMOTE**
>> ### **SMOTEEN**
> ## <br> **9.7: Model Performance Results**
>> ### **Performance Matrix**
>> ### **Performance Ranking**
# <br><br> **Section 10: Save Models To Files**
> ## <br> **10.1: Logistic Regression**
>> ### **Random Undersampling**
>> ### **Random Oversampling**
>> ### **Cluster Centroids**
>> ### **SMOTE**
>> ### **SMOTEEN**
> ## <br> **10.2: Decision Tree**
>> ### **Random Undersampling**
>> ### **Random Oversampling**
>> ### **Cluster Centroids**
>> ### **SMOTE**
>> ### **SMOTEEN**
> ## <br> **10.3: Random Forest**
>> ### **Random Undersampling**
>> ### **Random Oversampling**
>> ### **Cluster Centroids**
>> ### **SMOTE**
>> ### **SMOTEEN**
> ## <br> **10.4: Support Vector Machine (SVM)**
>> ### **Random Undersampling**
>> ### **Random Oversampling**
>> ### **Cluster Centroids**
>> ### **SMOTE**
>> ### **SMOTEEN**
> ## <br> **10.5: K-Nearest Neighbor (KNN)**
>> ### **Random Undersampling**
>> ### **Random Oversampling**
>> ### **Cluster Centroids**
>> ### **SMOTE**
>> ### **SMOTEEN**
> ## <br> **10.6: Gaussian Naive Bayes (GNB))**
>> ### **Random Undersampling**
>> ### **Random Oversampling**
>> ### **Cluster Centroids**
>> ### **SMOTE**
>> ### **SMOTEEN**

----

## Table of Contents (spam_detector_hyperparameters_optimization_colab)

----

# <br><br> **Section 1: Extraction and Transformation**
> ## <br> **1.1: Read the CSV data into a Pandas DataFrame**
> ## <br> **1.2: Display Spam DataFrame**
> ## <br> **1.3: Create the labels series (y) from the “spam” column, and then create the features (X) DataFrame from the remaining columns.**
>> ### **Separate the Y Variable, The Labels**
>> ### **Review the Y Series**
>> ### **Check the Balance of the Labels Variable (y) by Using the value_counts Function.**
>> ### **Separate the X Variable, the Features**
>> ### **Review the X DataFrame**
> ## <br> **1.4: Split the Data into Training and Testing Datasets by Using train_test_split.**
> ## <br> **1.5: Use the StandardScaler to Scale the X Variables**
>> ### **Scale Training and Test Data as Numpy Arrays**
>> ### **Create Scaled X Variable DataFrames**
>> ### **Display Scaled Training and Testing Data**
# <br><br> **Section 2: Undersampled and OverSampled Spam Data**
> ## <br> **2.1: Instantiate the Random Undersampler Instance**
> ## <br> **2.2: Instantiate the Random Oversampler Instance**
> ## <br> **2.4: Instantiate the SMOTE Instance**
> ## <br> **2.5: Instantiate the SMOTEENN Instance**
> ## <br> **2.6: Check the Balance of the Labels Variable (y) by Using the value_counts Function.**
> ## <br> **2.7: Display Normalized Resampled Training and Testing Data**
# <br><br> **Section 3: Model Optimization**
> ## <br> **3.1: Logistic Regression**
>> ### **Original**
>> ### **Random Undersampling**
>> ### **Random Oversampling**
>> ### **Cluster Centroids**
>> ### **SMOTE**
>> ### **SMOTEEN**
> ## <br> **3.2: Decision Tree**
>> ### **Original**
>> ### **Random Undersampling**
>> ### **Random Oversampling**
>> ### **Cluster Centroids**
>> ### **SMOTE**
>> ### **SMOTEEN**
> ## <br> **3.3: Random Forest**
>> ### **Original**
>> ### **Random Undersampling**
>> ### **Random Oversampling**
>> ### **Cluster Centroids**
>> ### **SMOTE**
>> ### **SMOTEEN**
> ## <br> **3.4: Support Vector Machine (SVM)**
>> ### **Original**
>> ### **Random Undersampling**
>> ### **Random Oversampling**
>> ### **Cluster Centroids**
>> ### **SMOTE**
>> ### **SMOTEEN**
> ## <br> **3.5: K-Nearest Neighbor (KNN)**
>> ### **Original**
>> ### **Random Undersampling**
>> ### **Random Oversampling**
>> ### **Cluster Centroids**
>> ### **SMOTE**
>> ### **SMOTEEN**
# <br><br> **Section 4: Save Grid Search Models To Files**
> ## <br> **4.1: Logistic Regression**
>> ### **Random Undersampling**
>> ### **Random Oversampling**
>> ### **Cluster Centroids**
>> ### **SMOTE**
>> ### **SMOTEEN**
> ## <br> **4.2: Decision Tree**
>> ### **Random Undersampling**
>> ### **Random Oversampling**
>> ### **Cluster Centroids**
>> ### **SMOTE**
>> ### **SMOTEEN**
> ## <br> **4.3: Random Forest**
>> ### **Random Undersampling**
>> ### **Random Oversampling**
>> ### **Cluster Centroids**
>> ### **SMOTE**
>> ### **SMOTEEN**
> ## <br> **4.4: Support Vector Machine (SVM)**
>> ### **Random Undersampling**
>> ### **Random Oversampling**
>> ### **Cluster Centroids**
>> ### **SMOTE**
>> ### **SMOTEEN**
> ## <br> **4.5: K-Nearest Neighbor (KNN)**
>> ### **Random Undersampling**
>> ### **Random Oversampling**
>> ### **Cluster Centroids**
>> ### **SMOTE**
>> ### **SMOTEEN**

----

## Copyright

Nicholas J. George © 2023. All Rights Reserved.
